{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ George's Dream Factory - FLUX KREA Pro Edition\\n",
    "\\n",
    "**Optimized for Google Colab Pro** with T5-v1.1-XXL text encoder and advanced features\\n",
    "\\n",
    "## Features:\\n",
    "- **T5-v1.1-XXL**: Google's best text encoder (11B parameters)\\n",
    "- **CLIP ViT-L/14**: Additional text encoding for better understanding\\n",
    "- **xFormers**: 50% memory reduction with faster attention\\n",
    "- **Batch Generation**: Process multiple images at once\\n",
    "- **Progress Tracking**: Real-time generation updates\\n",
    "- **VAE Slicing**: Generate HD images up to 2048px\\n",
    "\\n",
    "## Setup Instructions:\\n",
    "1. **Runtime**: Go to Runtime â†’ Change runtime type â†’ GPU â†’ A100/V100\\n",
    "2. **Run cells**: Execute all cells in order\\n",
    "3. **Get URL**: Copy the ngrok URL from the output\\n",
    "4. **Connect**: Paste URL in your frontend settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and system resources\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv\n",
    "print(\"\\nðŸ“Š System Information:\")\n",
    "!free -h\n",
    "!df -h /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install optimized dependencies\n",
    "print(\"ðŸ“¦ Installing dependencies optimized for Colab Pro...\")\n",
    "!pip install -q --upgrade pip\n",
    "\n",
    "# PyTorch with CUDA 11.8\n",
    "!pip install -q torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Core ML libraries\n",
    "!pip install -q diffusers==0.26.3 transformers==4.38.1 accelerate==0.27.2 sentencepiece==0.1.99\n",
    "\n",
    "# xFormers for memory optimization\n",
    "!pip install -q xformers==0.0.23 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# API and utilities\n",
    "!pip install -q fastapi uvicorn pyngrok python-multipart pillow\n",
    "!pip install -q safetensors einops omegaconf scipy ftfy beautifulsoup4\n",
    "\n",
    "print(\"âœ… All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set up environment\n",
    "import os\n",
    "import gc\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "import queue\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict, Any\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "from PIL import Image\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "import uvicorn\n",
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "from diffusers import FluxPipeline, DiffusionPipeline\n",
    "from transformers import T5EncoderModel, T5TokenizerFast, CLIPTextModel, CLIPTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"ðŸŒŸ FLUX KREA PRO - Enhanced with T5-XXL + CLIP\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Model settings\n",
    "    MODEL_ID = \"black-forest-labs/FLUX.1-schnell\"\n",
    "    T5_MODEL = \"google/t5-v1_1-xxl\"  # Best T5 model\n",
    "    CLIP_MODEL = \"openai/clip-vit-large-patch14\"\n",
    "    \n",
    "    # Server settings\n",
    "    PORT = 7860\n",
    "    ENABLE_XFORMERS = True\n",
    "    ENABLE_VAE_SLICING = True\n",
    "    ENABLE_CPU_OFFLOAD = False\n",
    "    \n",
    "    # Generation defaults (KREA optimized)\n",
    "    DEFAULT_STEPS = 30\n",
    "    DEFAULT_GUIDANCE = 4.0\n",
    "    DEFAULT_WIDTH = 1024\n",
    "    DEFAULT_HEIGHT = 1024\n",
    "    MAX_BATCH_SIZE = 4\n",
    "    \n",
    "    # Performance\n",
    "    USE_FLOAT16 = True\n",
    "    TORCH_COMPILE = False  # Set True for 30% speedup after warmup\n",
    "    \n",
    "    # Your ngrok token (get free at https://ngrok.com)\n",
    "    NGROK_TOKEN = None  # Replace with your token\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Set your ngrok token here or in the next cell\n",
    "# config.NGROK_TOKEN = \"your_token_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Model Manager with T5-XXL\n",
    "class FluxKreaModelManager:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.pipe = None\n",
    "        self.t5_encoder = None\n",
    "        self.t5_tokenizer = None\n",
    "        self.clip_model = None\n",
    "        self.clip_tokenizer = None\n",
    "        self.model_loaded = False\n",
    "        self.loading_progress = 0\n",
    "        self.loading_status = \"Not started\"\n",
    "        \n",
    "    def load_t5_xxl(self):\n",
    "        \"\"\"Load the T5-XXL model for superior text encoding\"\"\"\n",
    "        print(\"ðŸ“ Loading T5-v1.1-XXL text encoder (this is 11B parameters!)...\")\n",
    "        self.loading_status = \"Loading T5-XXL encoder\"\n",
    "        self.loading_progress = 10\n",
    "        \n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.t5_tokenizer = T5TokenizerFast.from_pretrained(\n",
    "                config.T5_MODEL,\n",
    "                model_max_length=512\n",
    "            )\n",
    "            \n",
    "            # Load model with optimization\n",
    "            self.t5_encoder = T5EncoderModel.from_pretrained(\n",
    "                config.T5_MODEL,\n",
    "                torch_dtype=torch.float16 if config.USE_FLOAT16 else torch.float32,\n",
    "                device_map=\"auto\",  # Automatically handle model sharding\n",
    "                load_in_8bit=False,  # Set True if running out of memory\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            self.loading_progress = 30\n",
    "            print(\"âœ… T5-XXL loaded successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading T5-XXL: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def load_clip(self):\n",
    "        \"\"\"Load CLIP model for additional text encoding\"\"\"\n",
    "        print(\"ðŸŽ¨ Loading CLIP text encoder...\")\n",
    "        self.loading_status = \"Loading CLIP encoder\"\n",
    "        self.loading_progress = 40\n",
    "        \n",
    "        try:\n",
    "            self.clip_tokenizer = CLIPTokenizer.from_pretrained(config.CLIP_MODEL)\n",
    "            self.clip_model = CLIPTextModel.from_pretrained(\n",
    "                config.CLIP_MODEL,\n",
    "                torch_dtype=torch.float16 if config.USE_FLOAT16 else torch.float32\n",
    "            ).to(self.device)\n",
    "            \n",
    "            self.loading_progress = 50\n",
    "            print(\"âœ… CLIP loaded successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading CLIP: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def load_flux_pipeline(self):\n",
    "        \"\"\"Load FLUX pipeline with custom text encoders\"\"\"\n",
    "        print(\"ðŸ”® Loading FLUX pipeline with enhanced text encoders...\")\n",
    "        self.loading_status = \"Loading FLUX model\"\n",
    "        self.loading_progress = 60\n",
    "        \n",
    "        try:\n",
    "            # Load FLUX with our custom encoders\n",
    "            self.pipe = FluxPipeline.from_pretrained(\n",
    "                config.MODEL_ID,\n",
    "                text_encoder=self.t5_encoder,\n",
    "                text_encoder_2=self.clip_model,\n",
    "                tokenizer=self.t5_tokenizer,\n",
    "                tokenizer_2=self.clip_tokenizer,\n",
    "                torch_dtype=torch.float16 if config.USE_FLOAT16 else torch.float32,\n",
    "                use_safetensors=True,\n",
    "                variant=\"fp16\" if config.USE_FLOAT16 else None\n",
    "            )\n",
    "            \n",
    "            self.pipe = self.pipe.to(self.device)\n",
    "            self.loading_progress = 80\n",
    "            \n",
    "            # Apply optimizations\n",
    "            if config.ENABLE_XFORMERS and self.device == \"cuda\":\n",
    "                self.pipe.enable_xformers_memory_efficient_attention()\n",
    "                print(\"âš¡ xFormers enabled - 50% memory reduction!\")\n",
    "            \n",
    "            if config.ENABLE_VAE_SLICING:\n",
    "                self.pipe.enable_vae_slicing()\n",
    "                print(\"ðŸ”ª VAE slicing enabled for HD images\")\n",
    "            \n",
    "            # Compile with Torch 2.0 for speed\n",
    "            if config.TORCH_COMPILE and hasattr(torch, 'compile'):\n",
    "                print(\"ðŸ”¥ Compiling model with Torch 2.0...\")\n",
    "                self.pipe.unet = torch.compile(self.pipe.unet, mode=\"reduce-overhead\")\n",
    "            \n",
    "            self.loading_progress = 100\n",
    "            self.loading_status = \"Ready\"\n",
    "            self.model_loaded = True\n",
    "            \n",
    "            print(\"âœ… FLUX pipeline ready with T5-XXL + CLIP!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading FLUX: {str(e)}\")\n",
    "            self.loading_status = f\"Error: {str(e)}\"\n",
    "            return False\n",
    "    \n",
    "    def load_all(self):\n",
    "        \"\"\"Load all models in sequence\"\"\"\n",
    "        success = (\n",
    "            self.load_t5_xxl() and\n",
    "            self.load_clip() and\n",
    "            self.load_flux_pipeline()\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            # Optimize memory\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            print(\"\\nðŸŽ‰ All models loaded successfully!\")\n",
    "            print(f\"ðŸ’¾ GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.1f}GB used\")\n",
    "        \n",
    "        return success\n",
    "\n",
    "# Initialize model manager\n",
    "model_manager = FluxKreaModelManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request models and queue system\n",
    "generation_queue = queue.Queue()\n",
    "results_cache = {}\n",
    "\n",
    "class GenerationRequest:\n",
    "    def __init__(self, request_id, prompt, params):\n",
    "        self.id = request_id\n",
    "        self.prompt = prompt\n",
    "        self.params = params\n",
    "        self.status = \"queued\"\n",
    "        self.progress = 0\n",
    "        self.result = None\n",
    "        self.error = None\n",
    "        self.created_at = time.time()\n",
    "\n",
    "# Pydantic models for API\n",
    "class GenerateRequest(BaseModel):\n",
    "    prompt: str = Field(..., description=\"Text prompt\")\n",
    "    steps: int = Field(30, ge=20, le=50)\n",
    "    cfg_guidance: float = Field(4.0, ge=1.0, le=10.0)\n",
    "    seed: int = Field(-1)\n",
    "    width: int = Field(1024, ge=512, le=2048)\n",
    "    height: int = Field(1024, ge=512, le=2048)\n",
    "    negative_prompt: str = Field(\"\", description=\"What to avoid\")\n",
    "\n",
    "class BatchGenerateRequest(BaseModel):\n",
    "    prompts: List[str] = Field(..., max_items=4)\n",
    "    steps: int = Field(30)\n",
    "    cfg_guidance: float = Field(4.0)\n",
    "    width: int = Field(1024)\n",
    "    height: int = Field(1024)\n",
    "    negative_prompt: str = Field(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FastAPI app with all endpoints\n",
    "app = FastAPI(title=\"FLUX KREA Pro API\", version=\"2.0\")\n",
    "\n",
    "# Configure CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        'name': 'FLUX KREA Pro API',\n",
    "        'version': '2.0',\n",
    "        'model': 'FLUX.1 with T5-XXL',\n",
    "        'features': [\n",
    "            'T5-v1.1-XXL text encoder (11B params)',\n",
    "            'CLIP ViT-L/14 encoder',\n",
    "            'Batch generation up to 4 images',\n",
    "            'Real-time progress tracking',\n",
    "            'xFormers memory optimization',\n",
    "            'VAE slicing for HD images'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        'status': 'healthy' if model_manager.model_loaded else 'loading',\n",
    "        'device': model_manager.device,\n",
    "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
    "        'memory': {\n",
    "            'allocated': f\"{torch.cuda.memory_allocated() / 1024**3:.1f}GB\",\n",
    "            'reserved': f\"{torch.cuda.memory_reserved() / 1024**3:.1f}GB\"\n",
    "        } if torch.cuda.is_available() else None\n",
    "    }\n",
    "\n",
    "@app.get(\"/status\")\n",
    "async def status_check():\n",
    "    return {\n",
    "        'status': model_manager.loading_status,\n",
    "        'progress': model_manager.loading_progress,\n",
    "        'model_loaded': model_manager.model_loaded,\n",
    "        'models_loaded': model_manager.model_loaded,\n",
    "        'models_loading': not model_manager.model_loaded and model_manager.loading_progress > 0,\n",
    "        'queue_size': generation_queue.qsize(),\n",
    "        'device': model_manager.device\n",
    "    }\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate_image(request: GenerateRequest):\n",
    "    if not model_manager.model_loaded:\n",
    "        raise HTTPException(503, \"Model is still loading\")\n",
    "    \n",
    "    request_id = str(uuid.uuid4())\n",
    "    gen_request = GenerationRequest(\n",
    "        request_id=request_id,\n",
    "        prompt=request.prompt,\n",
    "        params=request.dict()\n",
    "    )\n",
    "    \n",
    "    generation_queue.put(gen_request)\n",
    "    results_cache[request_id] = gen_request\n",
    "    \n",
    "    # Wait for result (with timeout)\n",
    "    timeout = 300  # 5 minutes\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < timeout:\n",
    "        if gen_request.status == \"completed\":\n",
    "            return {\n",
    "                'success': True,\n",
    "                'image': gen_request.result['images'][0],\n",
    "                'seed': gen_request.result['seed'],\n",
    "                'duration': gen_request.result['duration']\n",
    "            }\n",
    "        elif gen_request.status == \"failed\":\n",
    "            raise HTTPException(500, gen_request.error)\n",
    "        await asyncio.sleep(0.1)\n",
    "    \n",
    "    raise HTTPException(504, \"Generation timeout\")\n",
    "\n",
    "@app.post(\"/generate/batch\")\n",
    "async def generate_batch(request: BatchGenerateRequest):\n",
    "    if not model_manager.model_loaded:\n",
    "        raise HTTPException(503, \"Model is still loading\")\n",
    "    \n",
    "    request_ids = []\n",
    "    for prompt in request.prompts:\n",
    "        request_id = str(uuid.uuid4())\n",
    "        params = request.dict()\n",
    "        params['prompt'] = prompt\n",
    "        \n",
    "        gen_request = GenerationRequest(\n",
    "            request_id=request_id,\n",
    "            prompt=prompt,\n",
    "            params=params\n",
    "        )\n",
    "        \n",
    "        generation_queue.put(gen_request)\n",
    "        results_cache[request_id] = gen_request\n",
    "        request_ids.append(request_id)\n",
    "    \n",
    "    return {\n",
    "        'success': True,\n",
    "        'request_ids': request_ids,\n",
    "        'message': f'Queued {len(request.prompts)} images'\n",
    "    }\n",
    "\n",
    "@app.get(\"/progress/{request_id}\")\n",
    "async def get_progress(request_id: str):\n",
    "    if request_id not in results_cache:\n",
    "        raise HTTPException(404, \"Invalid request ID\")\n",
    "    \n",
    "    gen_request = results_cache[request_id]\n",
    "    \n",
    "    response = {\n",
    "        'request_id': request_id,\n",
    "        'status': gen_request.status,\n",
    "        'progress': gen_request.progress\n",
    "    }\n",
    "    \n",
    "    if gen_request.status == \"completed\":\n",
    "        response['result'] = gen_request.result\n",
    "    elif gen_request.status == \"failed\":\n",
    "        response['error'] = gen_request.error\n",
    "    \n",
    "    return response\n",
    "\n",
    "@app.get(\"/models\")\n",
    "async def get_models():\n",
    "    return {\n",
    "        'models': [{\n",
    "            'id': 'flux-krea-pro',\n",
    "            'name': 'FLUX KREA Pro with T5-XXL',\n",
    "            'status': 'loaded' if model_manager.model_loaded else 'loading',\n",
    "            'text_encoders': {\n",
    "                't5': 'google/t5-v1_1-xxl (11B params)',\n",
    "                'clip': 'openai/clip-vit-large-patch14'\n",
    "            }\n",
    "        }]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation worker function\n",
    "def generation_worker():\n",
    "    \"\"\"Process generation requests from queue\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            gen_request = generation_queue.get(timeout=1)\n",
    "            \n",
    "            if gen_request is None:\n",
    "                break\n",
    "            \n",
    "            print(f\"\\nðŸŽ¨ Generating: {gen_request.prompt[:60]}...\")\n",
    "            gen_request.status = \"processing\"\n",
    "            gen_request.progress = 10\n",
    "            \n",
    "            # Set up parameters\n",
    "            params = gen_request.params\n",
    "            \n",
    "            # Handle seed\n",
    "            if params['seed'] == -1:\n",
    "                params['seed'] = torch.randint(0, 1000000, (1,)).item()\n",
    "            \n",
    "            generator = torch.Generator(device=model_manager.device).manual_seed(params['seed'])\n",
    "            \n",
    "            # Generate\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                with torch.cuda.amp.autocast(enabled=True):\n",
    "                    gen_request.progress = 50\n",
    "                    \n",
    "                    # Generate with T5-XXL enhanced prompts\n",
    "                    result = model_manager.pipe(\n",
    "                        prompt=params['prompt'],\n",
    "                        negative_prompt=params.get('negative_prompt', ''),\n",
    "                        num_inference_steps=params['steps'],\n",
    "                        guidance_scale=params['cfg_guidance'],\n",
    "                        generator=generator,\n",
    "                        width=params['width'],\n",
    "                        height=params['height']\n",
    "                    )\n",
    "                    \n",
    "                    gen_request.progress = 90\n",
    "                    \n",
    "                    # Convert to base64\n",
    "                    images = []\n",
    "                    for img in result.images:\n",
    "                        buffered = io.BytesIO()\n",
    "                        img.save(buffered, format=\"PNG\", optimize=True)\n",
    "                        img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "                        images.append(f\"data:image/png;base64,{img_base64}\")\n",
    "                    \n",
    "                    duration = time.time() - start_time\n",
    "                    \n",
    "                    gen_request.result = {\n",
    "                        'images': images,\n",
    "                        'seed': params['seed'],\n",
    "                        'duration': duration\n",
    "                    }\n",
    "                    gen_request.status = \"completed\"\n",
    "                    gen_request.progress = 100\n",
    "                    \n",
    "                    print(f\"âœ… Generated in {duration:.1f}s | Seed: {params['seed']}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                gen_request.error = str(e)\n",
    "                gen_request.status = \"failed\"\n",
    "                print(f\"âŒ Error: {str(e)}\")\n",
    "            \n",
    "            # Cleanup\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        except queue.Empty:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Worker error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models in background\n",
    "print(\"ðŸ”„ Starting model loading...\")\n",
    "print(\"This will load:\")\n",
    "print(\"  â€¢ T5-v1.1-XXL (11B parameters)\")\n",
    "print(\"  â€¢ CLIP ViT-L/14\")\n",
    "print(\"  â€¢ FLUX pipeline\")\n",
    "print(\"\\nThis may take 3-5 minutes...\\n\")\n",
    "\n",
    "def load_models_background():\n",
    "    success = model_manager.load_all()\n",
    "    if success:\n",
    "        print(\"\\nâœ¨ FLUX KREA Pro is ready!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"ðŸš€ Enhanced with:\")\n",
    "        print(\"  â€¢ T5-v1.1-XXL for superior text understanding\")\n",
    "        print(\"  â€¢ xFormers for 50% memory savings\")\n",
    "        print(\"  â€¢ Batch processing support\")\n",
    "        print(\"  â€¢ Real-time progress tracking\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# Start loading\n",
    "model_thread = threading.Thread(target=load_models_background, daemon=True)\n",
    "model_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ngrok (optional but recommended)\n",
    "# Get your free auth token at: https://dashboard.ngrok.com/signup\n",
    "\n",
    "# Uncomment and add your token:\n",
    "# ngrok.set_auth_token(\"your_ngrok_auth_token_here\")\n",
    "\n",
    "# Or set it in environment\n",
    "# os.environ['NGROK_AUTH_TOKEN'] = \"your_token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start everything!\n",
    "print(\"ðŸŒ Setting up public access...\")\n",
    "\n",
    "# Create ngrok tunnel\n",
    "public_url = ngrok.connect(config.PORT)\n",
    "print(f\"\\nðŸš€ FLUX KREA Pro API is starting!\")\n",
    "print(f\"\\nðŸ”— Public URL: {public_url}\")\n",
    "print(f\"ðŸ“Š Status: {public_url}/status\")\n",
    "print(f\"ðŸ¥ Health: {public_url}/health\")\n",
    "print(f\"ðŸ“š Models: {public_url}/models\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ CONNECTION INSTRUCTIONS:\")\n",
    "print(\"1. Wait for models to load (check /status)\")\n",
    "print(\"2. Copy the public URL above\")\n",
    "print(\"3. Open your Dream Factory frontend\")\n",
    "print(\"4. Paste the URL in settings\")\n",
    "print(\"5. Start creating amazing images!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Start generation worker\n",
    "worker_thread = threading.Thread(target=generation_worker, daemon=True)\n",
    "worker_thread.start()\n",
    "\n",
    "# Run the server\n",
    "print(\"\\nðŸš€ Starting API server...\")\n",
    "print(\"âš ï¸  Keep this cell running!\\n\")\n",
    "\n",
    "config_uvicorn = uvicorn.Config(app, host=\"0.0.0.0\", port=config.PORT, log_level=\"info\")\n",
    "server = uvicorn.Server(config_uvicorn)\n",
    "await server.serve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test the API\n",
    "\n",
    "Run these cells to test your API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single generation\n",
    "import requests\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "# Use the public URL from above\n",
    "test_url = str(public_url)  # or \"http://localhost:7860\" for local testing\n",
    "\n",
    "# Check status first\n",
    "status = requests.get(f\"{test_url}/status\").json()\n",
    "print(\"Status:\", status)\n",
    "\n",
    "if status['model_loaded']:\n",
    "    # Test generation\n",
    "    response = requests.post(\n",
    "        f\"{test_url}/generate\",\n",
    "        json={\n",
    "            \"prompt\": \"A majestic snow leopard resting on a cliff at sunset, golden hour lighting, photorealistic, 8k quality\",\n",
    "            \"steps\": 30,\n",
    "            \"cfg_guidance\": 4.0,\n",
    "            \"width\": 1280,\n",
    "            \"height\": 1024,\n",
    "            \"negative_prompt\": \"blurry, low quality, distorted\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    result = response.json()\n",
    "    if result.get('success'):\n",
    "        print(f\"âœ… Generated in {result['duration']:.1f}s\")\n",
    "        print(f\"Seed: {result['seed']}\")\n",
    "        \n",
    "        # Display the image\n",
    "        img_data = base64.b64decode(result['image'].split(',')[1])\n",
    "        display(IPImage(img_data))\n",
    "    else:\n",
    "        print(\"Error:\", result)\n",
    "else:\n",
    "    print(\"â³ Model is still loading...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batch generation\n",
    "if model_manager.model_loaded:\n",
    "    response = requests.post(\n",
    "        f\"{test_url}/generate/batch\",\n",
    "        json={\n",
    "            \"prompts\": [\n",
    "                \"A serene Japanese garden with cherry blossoms\",\n",
    "                \"A futuristic cyberpunk cityscape at night\",\n",
    "                \"An enchanted forest with bioluminescent plants\"\n",
    "            ],\n",
    "            \"steps\": 28,\n",
    "            \"cfg_guidance\": 3.5,\n",
    "            \"width\": 1024,\n",
    "            \"height\": 1024\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    batch_result = response.json()\n",
    "    print(\"Batch result:\", batch_result)\n",
    "    \n",
    "    # Check progress\n",
    "    if batch_result.get('success'):\n",
    "        for req_id in batch_result['request_ids']:\n",
    "            progress = requests.get(f\"{test_url}/progress/{req_id}\").json()\n",
    "            print(f\"Request {req_id}: {progress['status']} ({progress['progress']}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Tips & Best Practices\n",
    "\n",
    "### Optimal Settings for FLUX KREA:\n",
    "- **Steps**: 28-32 (sweet spot for quality/speed)\n",
    "- **CFG Guidance**: 3.5-5.0 (lower than typical)\n",
    "- **Resolution**: 1024-1280px (best quality)\n",
    "\n",
    "### T5-XXL Prompt Tips:\n",
    "- Be descriptive and specific\n",
    "- Include style, lighting, and mood\n",
    "- T5 understands complex relationships\n",
    "- Use natural language, not just tags\n",
    "\n",
    "### Performance Optimization:\n",
    "- Enable xFormers (already enabled)\n",
    "- Use batch generation for multiple images\n",
    "- Keep resolution at 1024-1280 for best results\n",
    "- Reuse seeds for consistent styles\n",
    "\n",
    "### Troubleshooting:\n",
    "- If OOM: Reduce batch size or resolution\n",
    "- If slow: Check GPU type (A100 is best)\n",
    "- If quality issues: Increase steps to 32-40\n",
    "- If connection drops: Restart ngrok cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}