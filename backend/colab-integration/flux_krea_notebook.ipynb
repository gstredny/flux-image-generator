{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLUX Krea API Server for Image Generation\n",
    "\n",
    "This notebook sets up a FastAPI server that exposes the FLUX Krea model for image generation.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Run all cells in order\n",
    "2. The last cell will provide you with a public URL (via ngrok)\n",
    "3. Use this URL in your frontend application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q fastapi uvicorn pyngrok python-multipart pillow torch diffusers transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "import time\n",
    "import asyncio\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "from diffusers import FluxPipeline\n",
    "import uvicorn\n",
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "\n",
    "# Allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device and model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ID = \"black-forest-labs/FLUX.1-schnell\"  # Fast version of FLUX\n",
    "# For higher quality, use: \"black-forest-labs/FLUX.1-dev\" (requires auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "print(\"Loading FLUX model... This may take a few minutes on first run.\")\n",
    "\n",
    "try:\n",
    "    pipe = FluxPipeline.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    )\n",
    "    pipe = pipe.to(device)\n",
    "    \n",
    "    # Enable memory efficient attention if available\n",
    "    if hasattr(pipe, \"enable_attention_slicing\"):\n",
    "        pipe.enable_attention_slicing()\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Note: You may need to authenticate with Hugging Face for some models.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FastAPI app\n",
    "app = FastAPI(title=\"FLUX Krea API\", version=\"1.0.0\")\n",
    "\n",
    "# Configure CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # In production, replace with your frontend URL\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Request/Response models\n",
    "class GenerateRequest(BaseModel):\n",
    "    prompt: str = Field(..., description=\"Text prompt for image generation\")\n",
    "    steps: int = Field(30, ge=20, le=50, description=\"Number of inference steps\")\n",
    "    cfg_scale: float = Field(4.0, ge=1.0, le=10.0, description=\"CFG guidance scale\")\n",
    "    seed: int = Field(-1, description=\"Random seed (-1 for random)\")\n",
    "    width: int = Field(1024, ge=512, le=2048, description=\"Image width\")\n",
    "    height: int = Field(1024, ge=512, le=2048, description=\"Image height\")\n",
    "\n",
    "class GenerateResponse(BaseModel):\n",
    "    success: bool\n",
    "    image: Optional[str] = None\n",
    "    error: Optional[str] = None\n",
    "    duration: Optional[float] = None\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    status: str\n",
    "    model: str\n",
    "    device: str\n",
    "    version: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoints\n",
    "@app.get(\"/health\", response_model=HealthResponse)\n",
    "async def health_check():\n",
    "    \"\"\"Check if the API is healthy and model is loaded.\"\"\"\n",
    "    return HealthResponse(\n",
    "        status=\"healthy\",\n",
    "        model=MODEL_ID,\n",
    "        device=device,\n",
    "        version=\"1.0.0\"\n",
    "    )\n",
    "\n",
    "@app.post(\"/generate\", response_model=GenerateResponse)\n",
    "async def generate_image(request: GenerateRequest):\n",
    "    \"\"\"Generate an image from a text prompt.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Set random seed if requested\n",
    "        generator = None\n",
    "        if request.seed != -1:\n",
    "            generator = torch.Generator(device=device).manual_seed(request.seed)\n",
    "        \n",
    "        # Generate image\n",
    "        print(f\"Generating image with prompt: {request.prompt[:50]}...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image = pipe(\n",
    "                prompt=request.prompt,\n",
    "                num_inference_steps=request.steps,\n",
    "                guidance_scale=request.cfg_scale,\n",
    "                generator=generator,\n",
    "                width=request.width,\n",
    "                height=request.height,\n",
    "            ).images[0]\n",
    "        \n",
    "        # Convert to base64\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Image generated successfully in {duration:.2f}s\")\n",
    "        \n",
    "        return GenerateResponse(\n",
    "            success=True,\n",
    "            image=f\"data:image/png;base64,{img_base64}\",\n",
    "            duration=duration\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {str(e)}\")\n",
    "        return GenerateResponse(\n",
    "            success=False,\n",
    "            error=str(e),\n",
    "            duration=time.time() - start_time\n",
    "        )\n",
    "\n",
    "@app.get(\"/models\")\n",
    "async def get_models():\n",
    "    \"\"\"Get available models.\"\"\"\n",
    "    return {\"models\": [MODEL_ID]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ngrok auth token (optional but recommended for stable URLs)\n",
    "# You can get a free auth token from https://dashboard.ngrok.com/signup\n",
    "# Uncomment and add your token:\n",
    "# ngrok.set_auth_token(\"YOUR_NGROK_AUTH_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the server with ngrok\n",
    "import threading\n",
    "\n",
    "# Start ngrok tunnel\n",
    "public_url = ngrok.connect(7860)\n",
    "print(f\"\\nüöÄ API is running!\")\n",
    "print(f\"\\nüì± Public URL: {public_url}\")\n",
    "print(f\"\\nüîß Use this URL in your frontend application's VITE_API_ENDPOINT\")\n",
    "print(f\"\\nüìù API Documentation: {public_url}/docs\")\n",
    "print(f\"\\n‚ö° Health check: {public_url}/health\")\n",
    "\n",
    "# Run the server in a separate thread\n",
    "config = uvicorn.Config(app, host=\"0.0.0.0\", port=7860, log_level=\"info\")\n",
    "server = uvicorn.Server(config)\n",
    "\n",
    "thread = threading.Thread(target=server.run)\n",
    "thread.start()\n",
    "\n",
    "print(\"\\n‚úÖ Server is running! Keep this cell running to maintain the connection.\")\n",
    "print(\"\\n‚ö†Ô∏è  To stop the server, interrupt the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the API\n",
    "\n",
    "You can test the API using the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the API (optional)\n",
    "import requests\n",
    "\n",
    "# Use localhost for testing within Colab\n",
    "test_url = \"http://localhost:7860\"\n",
    "\n",
    "# Test health endpoint\n",
    "response = requests.get(f\"{test_url}/health\")\n",
    "print(\"Health check:\", response.json())\n",
    "\n",
    "# Test image generation\n",
    "test_prompt = \"A serene mountain landscape at sunset, photorealistic\"\n",
    "response = requests.post(\n",
    "    f\"{test_url}/generate\",\n",
    "    json={\n",
    "        \"prompt\": test_prompt,\n",
    "        \"steps\": 20,\n",
    "        \"cfg_scale\": 4.0,\n",
    "        \"seed\": 42,\n",
    "        \"width\": 1024,\n",
    "        \"height\": 1024\n",
    "    }\n",
    ")\n",
    "\n",
    "result = response.json()\n",
    "if result[\"success\"]:\n",
    "    print(f\"\\n‚úÖ Image generated successfully in {result['duration']:.2f}s\")\n",
    "    # Display the image\n",
    "    from IPython.display import Image as IPImage, display\n",
    "    import base64\n",
    "    img_data = base64.b64decode(result[\"image\"].split(\",\")[1])\n",
    "    display(IPImage(img_data))\n",
    "else:\n",
    "    print(f\"\\n‚ùå Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes:\n",
    "\n",
    "1. **GPU Runtime**: For best performance, use GPU runtime in Colab (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "\n",
    "2. **Model Options**:\n",
    "   - `FLUX.1-schnell`: Fast generation (4-8 steps), good quality\n",
    "   - `FLUX.1-dev`: Higher quality, requires more steps and HuggingFace authentication\n",
    "   - `FLUX.1-pro`: Best quality, requires API access\n",
    "\n",
    "3. **Memory Management**:\n",
    "   - The model uses ~6-8GB of VRAM\n",
    "   - Free Colab might disconnect after some time\n",
    "   - Consider Colab Pro for longer sessions\n",
    "\n",
    "4. **ngrok Limitations**:\n",
    "   - Free tier has request limits\n",
    "   - URLs change on restart\n",
    "   - Consider getting an auth token for stable URLs\n",
    "\n",
    "5. **Security**:\n",
    "   - The current CORS settings allow all origins\n",
    "   - In production, restrict to your frontend domain only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}