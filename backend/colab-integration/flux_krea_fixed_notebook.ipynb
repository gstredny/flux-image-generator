{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŽ¨ FLUX Image Generator - Fixed Version\n",
        "\n",
        "This notebook runs a FLUX model API server that works with George's Dream Factory frontend.\n",
        "\n",
        "**Important**: Make sure to use GPU runtime (Runtime â†’ Change runtime type â†’ T4 GPU)"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install all required dependencies\n",
        "!pip install torch diffusers transformers accelerate fastapi uvicorn pyngrok pillow -q\n",
        "!pip install pydantic --upgrade -q\n",
        "\n",
        "print(\"âœ… Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all required libraries and create the complete API server\n",
        "import os\n",
        "import base64\n",
        "import io\n",
        "import time\n",
        "import asyncio\n",
        "import logging\n",
        "from typing import Optional\n",
        "from datetime import datetime\n",
        "from threading import Thread\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from fastapi import FastAPI, HTTPException, Request\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from diffusers import FluxPipeline\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Model configuration\n",
        "MODEL_ID = \"black-forest-labs/FLUX.1-schnell\"  # Fast version\n",
        "\n",
        "# Global variables\n",
        "pipe = None\n",
        "device = None\n",
        "\n",
        "# Request/Response models with all fixes\n",
        "class GenerateRequest(BaseModel):\n",
        "    prompt: str = Field(..., min_length=1, max_length=1000, description=\"Text prompt for image generation\")\n",
        "    steps: int = Field(30, ge=20, le=50, description=\"Number of inference steps\")\n",
        "    cfg_scale: Optional[float] = Field(None, ge=1.0, le=10.0, description=\"CFG guidance scale (alias for cfg_guidance)\")\n",
        "    cfg_guidance: Optional[float] = Field(None, ge=1.0, le=10.0, description=\"CFG guidance scale\")\n",
        "    seed: int = Field(-1, ge=-1, le=999999999, description=\"Random seed (-1 for random)\")\n",
        "    width: int = Field(1024, ge=512, le=2048, description=\"Image width\")\n",
        "    height: int = Field(1024, ge=512, le=2048, description=\"Image height\")\n",
        "    \n",
        "    def __init__(self, **data):\n",
        "        # Handle both cfg_scale and cfg_guidance for backward compatibility\n",
        "        if 'cfg_scale' in data and 'cfg_guidance' not in data:\n",
        "            data['cfg_guidance'] = data['cfg_scale']\n",
        "        elif 'cfg_guidance' in data and 'cfg_scale' not in data:\n",
        "            data['cfg_scale'] = data['cfg_guidance']\n",
        "        elif 'cfg_scale' not in data and 'cfg_guidance' not in data:\n",
        "            data['cfg_scale'] = 4.0\n",
        "            data['cfg_guidance'] = 4.0\n",
        "        super().__init__(**data)\n",
        "        \n",
        "    @validator('prompt')\n",
        "    def validate_prompt(cls, v):\n",
        "        \"\"\"Validate prompt is not empty and doesn't contain harmful content.\"\"\"\n",
        "        if not v or not v.strip():\n",
        "            raise ValueError(\"Prompt cannot be empty\")\n",
        "        return v.strip()\n",
        "        \n",
        "    @validator('width', 'height')\n",
        "    def validate_dimensions(cls, v):\n",
        "        \"\"\"Ensure dimensions are multiples of 8 for better generation.\"\"\"\n",
        "        if v % 8 != 0:\n",
        "            # Round to nearest multiple of 8\n",
        "            v = round(v / 8) * 8\n",
        "            v = max(512, min(2048, v))  # Ensure within bounds\n",
        "        return v\n",
        "\n",
        "class GenerateResponse(BaseModel):\n",
        "    success: bool\n",
        "    image: Optional[str] = None\n",
        "    error: Optional[str] = None\n",
        "    duration: Optional[float] = None\n",
        "\n",
        "class HealthResponse(BaseModel):\n",
        "    status: str\n",
        "    model: str\n",
        "    device: str\n",
        "    version: str\n",
        "\n",
        "# Initialize model\n",
        "def initialize_model():\n",
        "    global pipe, device\n",
        "    \n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "    \n",
        "    if device == \"cuda\":\n",
        "        logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    \n",
        "    logger.info(\"Loading FLUX model... This may take a few minutes on first run.\")\n",
        "    \n",
        "    try:\n",
        "        pipe = FluxPipeline.from_pretrained(\n",
        "            MODEL_ID,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        )\n",
        "        pipe = pipe.to(device)\n",
        "        \n",
        "        # Enable memory efficient attention if available\n",
        "        if hasattr(pipe, \"enable_attention_slicing\"):\n",
        "            pipe.enable_attention_slicing()\n",
        "        \n",
        "        # Enable CPU offload for low memory systems\n",
        "        if device == \"cuda\" and torch.cuda.get_device_properties(0).total_memory < 8 * 1024**3:\n",
        "            logger.warning(\"Low GPU memory detected, enabling CPU offload...\")\n",
        "            pipe.enable_model_cpu_offload()\n",
        "        \n",
        "        logger.info(\"Model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading model: {e}\")\n",
        "        logger.error(\"Note: You may need to authenticate with Hugging Face for some models.\")\n",
        "        raise\n",
        "\n",
        "# Create FastAPI app\n",
        "app = FastAPI(title=\"FLUX Krea API\", version=\"1.0.0\")\n",
        "\n",
        "# Middleware for request logging\n",
        "@app.middleware(\"http\")\n",
        "async def log_requests(request: Request, call_next):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Log request\n",
        "    logger.info(f\"Incoming {request.method} request to {request.url.path}\")\n",
        "    \n",
        "    # Process request\n",
        "    response = await call_next(request)\n",
        "    \n",
        "    # Log response\n",
        "    duration = time.time() - start_time\n",
        "    logger.info(f\"Request to {request.url.path} completed in {duration:.2f}s with status {response.status_code}\")\n",
        "    \n",
        "    return response\n",
        "\n",
        "# Configure CORS\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Configure this for your frontend URL in production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    \"\"\"Initialize model on startup.\"\"\"\n",
        "    initialize_model()\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"Root endpoint.\"\"\"\n",
        "    return {\n",
        "        \"message\": \"FLUX Krea API Server\",\n",
        "        \"endpoints\": {\n",
        "            \"health\": \"/health\",\n",
        "            \"status\": \"/status\",\n",
        "            \"generate\": \"/generate\",\n",
        "            \"models\": \"/models\",\n",
        "            \"docs\": \"/docs\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\", response_model=HealthResponse)\n",
        "async def health_check():\n",
        "    \"\"\"Check if the API is healthy and model is loaded.\"\"\"\n",
        "    return HealthResponse(\n",
        "        status=\"healthy\" if pipe is not None else \"unhealthy\",\n",
        "        model=MODEL_ID,\n",
        "        device=str(device),\n",
        "        version=\"1.0.0\"\n",
        "    )\n",
        "\n",
        "@app.get(\"/status\")\n",
        "async def status_check():\n",
        "    \"\"\"Check model loading status and server health.\"\"\"\n",
        "    return {\n",
        "        \"status\": \"ok\" if pipe is not None else \"loading\",\n",
        "        \"model_loaded\": pipe is not None,\n",
        "        \"models_loaded\": pipe is not None,  # For backward compatibility\n",
        "        \"models_loading\": pipe is None,\n",
        "        \"message\": \"Model is ready\" if pipe is not None else \"Model is loading, please wait...\",\n",
        "        \"device\": str(device) if device else \"unknown\",\n",
        "        \"model\": MODEL_ID\n",
        "    }\n",
        "\n",
        "@app.post(\"/generate\", response_model=GenerateResponse)\n",
        "async def generate_image(request: GenerateRequest):\n",
        "    \"\"\"Generate an image from a text prompt.\"\"\"\n",
        "    if pipe is None:\n",
        "        raise HTTPException(\n",
        "            status_code=503,\n",
        "            detail=\"Model is still loading. Please wait a moment and try again.\"\n",
        "        )\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Set random seed if requested\n",
        "        generator = None\n",
        "        if request.seed != -1:\n",
        "            generator = torch.Generator(device=device).manual_seed(request.seed)\n",
        "        \n",
        "        # Generate image\n",
        "        logger.info(f\"Starting image generation\")\n",
        "        logger.info(f\"Prompt: {request.prompt[:100]}...\")\n",
        "        logger.info(f\"Parameters: steps={request.steps}, cfg_guidance={request.cfg_guidance}, size={request.width}x{request.height}, seed={request.seed}\")\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Run generation in thread pool to avoid blocking\n",
        "            loop = asyncio.get_event_loop()\n",
        "            image = await loop.run_in_executor(\n",
        "                None,\n",
        "                lambda: pipe(\n",
        "                    prompt=request.prompt,\n",
        "                    num_inference_steps=request.steps,\n",
        "                    guidance_scale=request.cfg_guidance,  # Use cfg_guidance which is normalized in __init__\n",
        "                    generator=generator,\n",
        "                    width=request.width,\n",
        "                    height=request.height,\n",
        "                ).images[0]\n",
        "            )\n",
        "        \n",
        "        # Convert to base64\n",
        "        buffered = io.BytesIO()\n",
        "        image.save(buffered, format=\"PNG\", optimize=True)\n",
        "        img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
        "        \n",
        "        duration = time.time() - start_time\n",
        "        logger.info(f\"Image generated successfully in {duration:.2f}s\")\n",
        "        logger.info(f\"Image size: {len(img_base64)} bytes (base64)\")\n",
        "        \n",
        "        return GenerateResponse(\n",
        "            success=True,\n",
        "            image=f\"data:image/png;base64,{img_base64}\",\n",
        "            duration=duration\n",
        "        )\n",
        "        \n",
        "    except torch.cuda.OutOfMemoryError:\n",
        "        logger.error(\"GPU out of memory error\")\n",
        "        return GenerateResponse(\n",
        "            success=False,\n",
        "            error=\"GPU out of memory. Try reducing image size or restarting the backend.\",\n",
        "            duration=time.time() - start_time\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating image: {type(e).__name__}: {str(e)}\")\n",
        "        error_msg = str(e)\n",
        "        \n",
        "        # Provide more helpful error messages\n",
        "        if \"CUDA\" in error_msg:\n",
        "            error_msg = \"GPU error. Please check CUDA availability and try again.\"\n",
        "        elif \"dimension\" in error_msg.lower() or \"size\" in error_msg.lower():\n",
        "            error_msg = f\"Invalid image dimensions. Please use sizes between 512 and 2048. Error: {error_msg}\"\n",
        "        elif \"prompt\" in error_msg.lower():\n",
        "            error_msg = \"Invalid prompt. Please check your input text.\"\n",
        "            \n",
        "        return GenerateResponse(\n",
        "            success=False,\n",
        "            error=error_msg,\n",
        "            duration=time.time() - start_time\n",
        "        )\n",
        "\n",
        "@app.get(\"/models\")\n",
        "async def get_models():\n",
        "    \"\"\"Get available models.\"\"\"\n",
        "    return {\"models\": [MODEL_ID]}\n",
        "\n",
        "print(\"\\nðŸŽ¨ FLUX Image Generator API - Fixed Version\")\n",
        "print(\"=\" * 50)\n",
        "print(\"âœ… All fixes applied:\")\n",
        "print(\"   - Added /status endpoint\")\n",
        "print(\"   - Fixed parameter mapping (cfg_scale/cfg_guidance)\")\n",
        "print(\"   - Enhanced error handling\")\n",
        "print(\"   - Added request/response logging\")\n",
        "print(\"   - Improved input validation\")\n",
        "print(\"=\" * 50)\n"
      ],
      "metadata": {
        "id": "main_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup ngrok for public access\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "\n",
        "# Allow nested event loops in Jupyter\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Kill any existing ngrok processes\n",
        "ngrok.kill()\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(7860)\n",
        "print(f\"\\nðŸš€ Public URL: {public_url}\")\n",
        "print(f\"\\nðŸ“‹ Copy this URL to your frontend settings!\")\n",
        "print(f\"\\nðŸ“ API Documentation: {public_url}/docs\")\n",
        "print(\"\\n\" + \"=\" * 50)"
      ],
      "metadata": {
        "id": "ngrok_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the server\n",
        "print(\"\\nðŸŒ Starting FLUX API Server...\")\n",
        "print(\"\\nâ³ First image generation will take 2-3 minutes while model loads.\")\n",
        "print(\"\\nâœ¨ Server is running! Use the ngrok URL above in your frontend.\")\n",
        "print(\"\\nPress 'Stop' button to shutdown the server.\")\n",
        "\n",
        "# Run the FastAPI server\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=7860)"
      ],
      "metadata": {
        "id": "run_server"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Œ How to Use:\n",
        "\n",
        "1. **Run all cells** in order (Runtime â†’ Run all)\n",
        "2. **Copy the ngrok URL** from the output (looks like `https://xxxx-xxxx.ngrok.io`)\n",
        "3. **Open your frontend** at http://localhost:5173\n",
        "4. **Go to Settings** and paste the ngrok URL\n",
        "5. **Click Test Connection** - it should show \"Dreams Ready!\"\n",
        "6. **Start generating images!**\n",
        "\n",
        "## ðŸ› ï¸ Troubleshooting:\n",
        "\n",
        "- **Connection refused**: Make sure all cells have run successfully\n",
        "- **Model loading**: First generation takes 2-3 minutes\n",
        "- **GPU memory**: If you get memory errors, restart runtime and try smaller images\n",
        "- **Ngrok expired**: Re-run the ngrok cell to get a new URL"
      ],
      "metadata": {
        "id": "instructions"
      }
    }
  ]
}